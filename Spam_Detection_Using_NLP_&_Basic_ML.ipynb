{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Detection Using NLP & Basic ML",
      "provenance": [],
      "authorship_tag": "ABX9TyOIIw4FPRQS4Au6kzlQIWTV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nileshgode/My-Python-Projects/blob/master/Spam_Detection_Using_NLP_%26_Basic_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4_TQ0FNJBlv",
        "colab_type": "text"
      },
      "source": [
        "Import the Necessary libraries for the task.\n",
        "numpy & pd as common for every project but as we are dealing with text data so here we use NLTK library to find the solution for problem statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDsvujjFHEd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoXLm-3zJgDt",
        "colab_type": "text"
      },
      "source": [
        "While working on google colab, We first need to mount the drive every time, enter the passcode, before that never forget to insert your data inside the drive while using colab or when working on Jupyter locally add data set in your working directory or change the path with command \"os.chdir\" to locate dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6SkKxhoHe1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86a15034-050a-4063-a04f-01c66d5519ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P26IeORbKG5Q",
        "colab_type": "text"
      },
      "source": [
        "Then read the data set with Pandas library function.\n",
        "do not forget to copy your data file path from google colab to read it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMhl9V9DH3kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/sample_data/spam.csv', encoding='latin1')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c23XJfKcDg",
        "colab_type": "text"
      },
      "source": [
        "### Data Cleaning & Data Understanding Steps \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFWeSwriKsIK",
        "colab_type": "text"
      },
      "source": [
        " df.head() : This Function give us o/p as first 5 Rows, If we wants more numbers of rows we can initialize the desire numbers of rows inside bracket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7omxubVYIzYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "66b24d4a-b1b3-4ee5-fef2-5ea9a1cd0759"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>text</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type  ... Unnamed: 4\n",
              "0   ham  ...        NaN\n",
              "1   ham  ...        NaN\n",
              "2  spam  ...        NaN\n",
              "3   ham  ...        NaN\n",
              "4   ham  ...        NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk1Aho1ZNn2K",
        "colab_type": "text"
      },
      "source": [
        "As We Can see cloumns Unnamed: 2\tUnnamed: 3\tUnnamed: 4 is not having any information so that we can drop those coulumns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g9kzI-KI86Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3a835a61-083f-480c-be3e-97d93bc2b49b"
      },
      "source": [
        "df=df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
        "df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_apDngr4N72e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5af515c7-618b-48ea-c7d3-3c780188b746"
      },
      "source": [
        "# See other commands to understand our Data\n",
        "# Printing the size of the dataset\n",
        "df.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byJJgzF_OKKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8da7a9e0-454b-4772-c370-805654a771a1"
      },
      "source": [
        "# Getting feature names\n",
        "df.columns"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['type', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcSQyFpPOQE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16211f17-3ea1-4989-b4b5-45bddd1dc3ad"
      },
      "source": [
        "# Checking the duplicates and remove them\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5169, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmaRBFuOaa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "805cbb88-f9a4-42c4-aef4-e65b08ba594c"
      },
      "source": [
        "# Show the number of missing data for each column\n",
        "df.isnull().sum()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "type    0\n",
              "text    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb9HYfHmN7Ur",
        "colab_type": "text"
      },
      "source": [
        "### Processing our text data with NLP\n",
        "Now our remaning data which is clean but it is in text format, for machine understanding will have to convert that data into Numerical form. We already have the library for tokenizing the words, Import NLTK and perform operation as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-S6M3BUO_Ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzMCZGCPPGN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to tokenize each and every word\n",
        "def tokenizer(text):\n",
        "    tokenized=nltk.word_tokenize(text)\n",
        "    tokenized=' '.join(tokenized)\n",
        "    tokenized=tokenized.replace('n\\'t','not')\n",
        "    return tokenized"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0gV7YYrPuxZ",
        "colab_type": "text"
      },
      "source": [
        "After Tokenization we need to remove punctuation, Remove stopwords with reference to stopwords stored in NLTK stopwords, Function just compare words within dictionary, if mathces remove it from the sentence, convert all words into lower case then return a list of clean words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxbqLT0oPUkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a function to process punctuation and stopwords in the text data\n",
        "def process_stop_punc(text):\n",
        "    # Remove punctuations\n",
        "    # Remove stopwords\n",
        "    # Return a list of clen text words\n",
        "    nopunc=[char for char in text if char not in string.punctuation]\n",
        "    nopunc=''.join(nopunc)\n",
        "    \n",
        "    clean_words=[word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    return clean_words"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejWlTCdFQh1R",
        "colab_type": "text"
      },
      "source": [
        "After above process will have to converts words into its base form called as stemming. this task done by following stemming() function here we use porterStemmer(), Its is the part of term normalization in NLP process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gwxyzkAPmWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to convert words into single form i.e. converting plural to singular and past ,past continuous to present\n",
        "def stemming(List):\n",
        "    stem_obj=nltk.stem.PorterStemmer()\n",
        "    List=[stem_obj.stem(i) for i in List]\n",
        "    message=' '.join(List)\n",
        "    return message"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeHD9VULRX37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to compile each and every operation\n",
        "def process(text):\n",
        "    return stemming(process_stop_punc(tokenizer(text)))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJA7RHhvRi5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "13c4c9bb-f60e-40e7-b9b8-f2ea82586ef2"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNjn2-hQRZ-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "98acc83a-dd47-4c48-ee13-0e9504d3b3f1"
      },
      "source": [
        "# Show the tokenization\n",
        "df['text'].head().apply(process)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Go jurong point crazi avail bugi n great world...\n",
              "1                                Ok lar joke wif u oni\n",
              "2    free entri 2 wkli comp win FA cup final tkt 21...\n",
              "3                  U dun say earli hor U c alreadi say\n",
              "4                 nah think goe usf live around though\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hq3Qx2ZR3he",
        "colab_type": "text"
      },
      "source": [
        "### Vectorizing the words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbBCUh2zWBmY",
        "colab_type": "text"
      },
      "source": [
        "TFIDFVectorizer the value increases proportionally to count, but is inversely proportional to frequency of the word in the corpus; that is the inverse document frequency (IDF) part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uffp9sJ4WTGT",
        "colab_type": "text"
      },
      "source": [
        "**TfidfVectorizer** and **CountVectorizer** both are methods for converting text data into vectors as model can process only numerical data.\n",
        "\n",
        "In **CountVectorizer** we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
        "\n",
        "To overcome this , we use TfidfVectorizer .\n",
        "\n",
        "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLviq20nRzJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert a collection of data to matrix of tokens using tf-idf vectorizer\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "message = TfidfVectorizer().fit_transform(df['text'])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm011uW3R4-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b90dde0e-56ff-4c86-c1fa-9b3e6088c4c9"
      },
      "source": [
        "# Getting the shape of message\n",
        "message.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5169, 8672)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZb9VBKPTy2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "27d46ff1-ff42-421a-9abc-2f49d21e42ab"
      },
      "source": [
        "# Print how our data look like in Numerical format with tf-idf.\n",
        "print(message)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 8267)\t0.1820760415281772\n",
            "  (0, 1069)\t0.32544292157369786\n",
            "  (0, 3594)\t0.15240463847472757\n",
            "  (0, 7645)\t0.15605579719351925\n",
            "  (0, 2048)\t0.27450748091103355\n",
            "  (0, 1749)\t0.31054526020101475\n",
            "  (0, 4476)\t0.27450748091103355\n",
            "  (0, 8489)\t0.22981449679298432\n",
            "  (0, 3634)\t0.18170677054225734\n",
            "  (0, 1751)\t0.27450748091103355\n",
            "  (0, 4087)\t0.1080194309412782\n",
            "  (0, 5537)\t0.15773893821302193\n",
            "  (0, 1303)\t0.2468122813993541\n",
            "  (0, 2327)\t0.2514110448509606\n",
            "  (0, 5920)\t0.25394599154794606\n",
            "  (0, 4350)\t0.32544292157369786\n",
            "  (0, 8030)\t0.2284782712166139\n",
            "  (0, 3550)\t0.1474570544871208\n",
            "  (1, 5533)\t0.5464988818914979\n",
            "  (1, 8392)\t0.4304438402468376\n",
            "  (1, 4318)\t0.5233434480300876\n",
            "  (1, 4512)\t0.406925248497845\n",
            "  (1, 5504)\t0.2767319100209511\n",
            "  (2, 77)\t0.2326251973903166\n",
            "  (2, 1156)\t0.16331528331958853\n",
            "  :\t:\n",
            "  (5167, 1786)\t0.2820992149566908\n",
            "  (5167, 3470)\t0.2744008686738812\n",
            "  (5167, 2892)\t0.24290552468890048\n",
            "  (5167, 7049)\t0.20395814718823002\n",
            "  (5167, 1778)\t0.13673277359621147\n",
            "  (5167, 8065)\t0.21062041399707843\n",
            "  (5167, 2592)\t0.18469635293243075\n",
            "  (5167, 5334)\t0.20868573103969204\n",
            "  (5167, 1438)\t0.14288820286282247\n",
            "  (5167, 7627)\t0.10319532003279058\n",
            "  (5167, 3308)\t0.12215409504489928\n",
            "  (5167, 7039)\t0.18503435583866787\n",
            "  (5167, 4615)\t0.15982569695504117\n",
            "  (5167, 1084)\t0.11232294630116563\n",
            "  (5167, 8313)\t0.19089150993177975\n",
            "  (5167, 4218)\t0.12281898312072442\n",
            "  (5167, 3781)\t0.17097956584622562\n",
            "  (5167, 7756)\t0.08437843735148565\n",
            "  (5167, 3358)\t0.16237204914715464\n",
            "  (5167, 4087)\t0.11278484851691671\n",
            "  (5168, 6505)\t0.5493950047150747\n",
            "  (5168, 7885)\t0.434678956678875\n",
            "  (5168, 4225)\t0.5770885193248134\n",
            "  (5168, 5244)\t0.39278764302749264\n",
            "  (5168, 7756)\t0.14800689768753802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPnKoxYoV26b",
        "colab_type": "text"
      },
      "source": [
        "CountVectorizer counts the word frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-YWAegKVZe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "848f518c-74df-4656-d01b-d31d32e6d75e"
      },
      "source": [
        "# Using countvectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "message1=CountVectorizer().fit_transform(df['text'])\n",
        "message1"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5169x8672 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 68018 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzmdFgouTchJ",
        "colab_type": "text"
      },
      "source": [
        "### Splitting data into training tesing set\n",
        "Our textual data is ready for model building, now with sklearn function we will split that into 80:20 pattern for training as well as testing resp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa9A8PP4SEN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the data into 80:20 train test ratio for dataset vectorized using tf-idfvectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(message,df['type'],test_size=0.2,random_state=123)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJVzdZcYUDIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "5acc004d-c420-40c0-8068-e229a1f15afa"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 8322)\t0.3824156699859192\n",
            "  (0, 2839)\t0.364909991486263\n",
            "  (0, 3556)\t0.2640817780717849\n",
            "  (0, 3526)\t0.27167573719830423\n",
            "  (0, 4675)\t0.30505768137040207\n",
            "  (0, 7049)\t0.22953717497514553\n",
            "  (0, 5117)\t0.2377858540441012\n",
            "  (0, 3532)\t0.2161881827275621\n",
            "  (0, 7613)\t0.2265167607990493\n",
            "  (0, 5477)\t0.1442902004783509\n",
            "  (0, 4848)\t0.2157313074007234\n",
            "  (0, 5525)\t0.14857924246881726\n",
            "  (0, 5133)\t0.2152792161966106\n",
            "  (0, 5570)\t0.15797411905848488\n",
            "  (0, 3381)\t0.2530769796690938\n",
            "  (0, 5367)\t0.16411571836579125\n",
            "  (0, 1084)\t0.12640971755377062\n",
            "  (0, 7756)\t0.09496060052263017\n",
            "  (1, 1909)\t0.6851217051595304\n",
            "  (1, 8594)\t0.5627094801353876\n",
            "  (1, 8259)\t0.38284923124418946\n",
            "  (1, 4087)\t0.25960114834259196\n",
            "  (2, 8397)\t0.39148547324989375\n",
            "  (2, 8016)\t0.3203510753357125\n",
            "  (2, 6729)\t0.3302136379412883\n",
            "  :\t:\n",
            "  (4133, 6352)\t0.29436287616434814\n",
            "  (4133, 4988)\t0.29436287616434814\n",
            "  (4133, 8364)\t0.2808879527320607\n",
            "  (4133, 6891)\t0.25272949084534074\n",
            "  (4133, 3388)\t0.2296939570877491\n",
            "  (4133, 4357)\t0.21042470389698312\n",
            "  (4133, 2471)\t0.16605827927372932\n",
            "  (4133, 6636)\t0.20912127211917603\n",
            "  (4133, 8372)\t0.18422170307626204\n",
            "  (4133, 4225)\t0.14250210953688655\n",
            "  (4133, 8402)\t0.12528551364724008\n",
            "  (4133, 6105)\t0.24437749554218163\n",
            "  (4133, 1438)\t0.12378149001052915\n",
            "  (4133, 8433)\t0.12611638818166637\n",
            "  (4133, 5405)\t0.23858965466678428\n",
            "  (4133, 8609)\t0.07411350493744946\n",
            "  (4133, 8480)\t0.2002059879018244\n",
            "  (4134, 5075)\t0.5231747201160274\n",
            "  (4134, 3857)\t0.36246238787256896\n",
            "  (4134, 2150)\t0.3103003763281836\n",
            "  (4134, 1234)\t0.37535603946124474\n",
            "  (4134, 4225)\t0.3136340672780499\n",
            "  (4134, 2936)\t0.4548362270351441\n",
            "  (4134, 8609)\t0.16311702380621446\n",
            "  (4134, 7756)\t0.16087655100558781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuoo1rY_UacR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "cf1e443d-67c4-4844-b09c-7a5b037a28bd"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1890)\t0.4834563071796509\n",
            "  (0, 1321)\t0.17279887941543384\n",
            "  (0, 7647)\t0.17755713104029425\n",
            "  (0, 8328)\t0.19064508663303867\n",
            "  (0, 3004)\t0.1687483717534898\n",
            "  (0, 4729)\t0.18176403366933422\n",
            "  (0, 1524)\t0.22281157988754302\n",
            "  (0, 2471)\t0.1363655694637619\n",
            "  (0, 3532)\t0.1366543641911891\n",
            "  (0, 1498)\t0.15799839013168815\n",
            "  (0, 3503)\t0.10109046077266595\n",
            "  (0, 8315)\t0.3304441482998072\n",
            "  (0, 7963)\t0.2987305529692703\n",
            "  (0, 5477)\t0.09120704636403047\n",
            "  (0, 8289)\t0.10213559883209336\n",
            "  (0, 8537)\t0.13940101126191806\n",
            "  (0, 5606)\t0.12718295711053096\n",
            "  (0, 1889)\t0.29098157559138044\n",
            "  (0, 7669)\t0.10516622020330577\n",
            "  (0, 2109)\t0.3159967802633763\n",
            "  (0, 7627)\t0.07341140422977838\n",
            "  (0, 3770)\t0.09142757966163703\n",
            "  (0, 1084)\t0.07990464308434181\n",
            "  (0, 7756)\t0.060025392340647576\n",
            "  (0, 3594)\t0.11320108509005652\n",
            "  :\t:\n",
            "  (1033, 6568)\t0.31431290962989383\n",
            "  (1033, 226)\t0.31431290962989383\n",
            "  (1033, 333)\t0.2383714044966735\n",
            "  (1033, 6861)\t0.21274602978338117\n",
            "  (1033, 2752)\t0.20800758986436743\n",
            "  (1033, 8316)\t0.26093983953684613\n",
            "  (1033, 4519)\t0.1861773660302921\n",
            "  (1033, 2232)\t0.19245585615881025\n",
            "  (1033, 3666)\t0.2054412486987212\n",
            "  (1033, 7896)\t0.2054412486987212\n",
            "  (1033, 8289)\t0.1328042959372444\n",
            "  (1033, 1183)\t0.1246522476318023\n",
            "  (1033, 7621)\t0.11924266548507673\n",
            "  (1033, 8464)\t0.1829700345070741\n",
            "  (1033, 8055)\t0.19188807464568688\n",
            "  (1033, 8096)\t0.22764479016363362\n",
            "  (1033, 2120)\t0.21821615722293008\n",
            "  (1033, 1813)\t0.1213394703981072\n",
            "  (1033, 2054)\t0.17806768858064853\n",
            "  (1033, 6073)\t0.1861773660302921\n",
            "  (1033, 816)\t0.2572010597378161\n",
            "  (1033, 3770)\t0.11888093363187631\n",
            "  (1033, 8609)\t0.15827288877795387\n",
            "  (1033, 7756)\t0.07804947598399645\n",
            "  (1033, 5537)\t0.1523443324313862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_FET3JiUkmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting the data into 80:20 train test ratio for dataset vectorized using countvectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train1,X_test1,y_train1,y_test1=train_test_split(message1,df['type'],test_size=0.2,random_state=0)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spXLe5u4VexJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTZ_ZyteW2Va",
        "colab_type": "text"
      },
      "source": [
        "### Model building with different algorithms\n",
        "\n",
        "1> Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Ebb3v7W8GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating and training the naive bayes classifier for dataset vectorized using tf-idfvectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier=MultinomialNB().fit(X_train,y_train)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Eu8GxVXJg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f35a259b-3be9-4fc1-c16e-2e6c80eef5f2"
      },
      "source": [
        "# Evaluate the model and training dataset\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "pred=classifier.predict(X_train)\n",
        "print(classification_report(y_train,pred))\n",
        "print()\n",
        "print('confusion Matrix:\\n',confusion_matrix(y_train,pred))\n",
        "print()\n",
        "print(' training accuracy score:\\n',accuracy_score(y_train,pred))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98      3628\n",
            "        spam       1.00      0.72      0.84       507\n",
            "\n",
            "    accuracy                           0.97      4135\n",
            "   macro avg       0.98      0.86      0.91      4135\n",
            "weighted avg       0.97      0.97      0.96      4135\n",
            "\n",
            "\n",
            "confusion Matrix:\n",
            " [[3628    0]\n",
            " [ 142  365]]\n",
            "\n",
            " training accuracy score:\n",
            " 0.965659008464329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mBoPSxyZQWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "9432371b-4677-4b1f-dfe3-5a0953b626a0"
      },
      "source": [
        "# Printing the predictions\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "pred=classifier.predict(X_test)\n",
        "print(classification_report(y_test,pred))\n",
        "print()\n",
        "print('confusion Matrix:\\n',confusion_matrix(y_test,pred))\n",
        "print()\n",
        "print('testing accuracy score:\\n',accuracy_score(y_test,pred))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.94      1.00      0.97       888\n",
            "        spam       1.00      0.63      0.77       146\n",
            "\n",
            "    accuracy                           0.95      1034\n",
            "   macro avg       0.97      0.82      0.87      1034\n",
            "weighted avg       0.95      0.95      0.94      1034\n",
            "\n",
            "\n",
            "confusion Matrix:\n",
            " [[888   0]\n",
            " [ 54  92]]\n",
            "\n",
            "testing accuracy score:\n",
            " 0.9477756286266924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og-x8AgwadJb",
        "colab_type": "text"
      },
      "source": [
        "From Above two Results we can say that our model is not overfitting as we got 96.56 % Accuracy on training and 94.77% on testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAI7f1Ahaut3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating and training the naive bayes classifier for dataset vectorized using Countvectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier=MultinomialNB().fit(X_train1,y_train1)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKqqOYSoa-tA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b9f2510f-a3e2-4258-d421-ab11d05e5543"
      },
      "source": [
        "# Evaluate the model and training dataset on Count Vectorizer\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "pred=classifier.predict(X_train1)\n",
        "print(classification_report(y_train1,pred))\n",
        "print()\n",
        "print('confusion Matrix:\\n',confusion_matrix(y_train1,pred))\n",
        "print()\n",
        "print(' training accuracy score:\\n',accuracy_score(y_train1,pred))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      1.00      1.00      3631\n",
            "        spam       0.98      0.97      0.97       504\n",
            "\n",
            "    accuracy                           0.99      4135\n",
            "   macro avg       0.99      0.98      0.99      4135\n",
            "weighted avg       0.99      0.99      0.99      4135\n",
            "\n",
            "\n",
            "confusion Matrix:\n",
            " [[3623    8]\n",
            " [  17  487]]\n",
            "\n",
            " training accuracy score:\n",
            " 0.9939540507859734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddiNO9WabLcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "07b83426-b799-4de7-97cb-09369786745e"
      },
      "source": [
        "# Printing the predictions for CountVectorizer \n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "pred=classifier.predict(X_test1)\n",
        "print(classification_report(y_test1,pred))\n",
        "print()\n",
        "print('confusion Matrix:\\n',confusion_matrix(y_test1,pred))\n",
        "print()\n",
        "print('testing accuracy score:\\n',accuracy_score(y_test1,pred))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99       885\n",
            "        spam       0.91      0.93      0.92       149\n",
            "\n",
            "    accuracy                           0.98      1034\n",
            "   macro avg       0.95      0.96      0.96      1034\n",
            "weighted avg       0.98      0.98      0.98      1034\n",
            "\n",
            "\n",
            "confusion Matrix:\n",
            " [[872  13]\n",
            " [ 10 139]]\n",
            "\n",
            "testing accuracy score:\n",
            " 0.9777562862669246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFnvIxpEbk8v",
        "colab_type": "text"
      },
      "source": [
        "Here we have compare both the results with tf-idf vectorizer and Count Vectorizer we get better accuracy results on Count Vectorizer. \n",
        "\n",
        "Let us try with SVM with grid Search approach to tune Hyperparameters\n",
        "\n",
        "Lets try on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7_bSPvbbT8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "28cd6c3c-6db1-4b6a-e169-b245b962b39c"
      },
      "source": [
        "# Prediction using LinearSVC and GridsearchCV and tokens obtained fron TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid={'C':[0.1,1,10,100]}\n",
        "grid=GridSearchCV(LinearSVC(),param_grid,refit=True)\n",
        "grid.fit(X_train,y_train)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None, param_grid={'C': [0.1, 1, 10, 100]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDxpDoorcwsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19c8f541-fb95-4fe7-9001-a7b607045955"
      },
      "source": [
        "#finding best C for best parameter\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32A9gjtReebt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06a8c3a7-e390-46f0-fbf6-45a1ea0b39e4"
      },
      "source": [
        "# Finding best accuracy\n",
        "print(grid.best_score_)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9818621523579203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uS-obAWehuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction of test data\n",
        "pred2=grid.predict(X_test)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtZ3Jnqoem_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e424e6ae-0d4e-435a-a819-a70ebb8a996f"
      },
      "source": [
        "# Evaluate the model and training dataset\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "print(classification_report(y_test,pred2))\n",
        "print()\n",
        "print('confusion Matrix:\\n',confusion_matrix(y_test,pred2))\n",
        "print()\n",
        "print('accuracy score:\\n',accuracy_score(y_test,pred2))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.99      0.99       888\n",
            "        spam       0.96      0.86      0.91       146\n",
            "\n",
            "    accuracy                           0.98      1034\n",
            "   macro avg       0.97      0.93      0.95      1034\n",
            "weighted avg       0.98      0.98      0.98      1034\n",
            "\n",
            "\n",
            "confusion Matrix:\n",
            " [[883   5]\n",
            " [ 20 126]]\n",
            "\n",
            "accuracy score:\n",
            " 0.9758220502901354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ5AmQm_etMV",
        "colab_type": "text"
      },
      "source": [
        "For **TF-IDF Vectorizer** data, we get better Accuracy score with SVM both on training (98.18 %) & Testing (97.58 %)\n",
        "\n",
        "\n",
        "Now Lets Chech Accuracy Score with Count Vectoriser data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuFXaSK2fRW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "076b25d7-413b-43cd-b969-627d91b774d2"
      },
      "source": [
        "# Prediction using LinearSVC and GridsearchCV and tokens obtained fron CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid1={'C':[0.1,1,10,100]}\n",
        "grid1=GridSearchCV(LinearSVC(),param_grid,refit=True)\n",
        "grid1.fit(X_train1,y_train1)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None, param_grid={'C': [0.1, 1, 10, 100]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhb3e3o9fXIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1acfea5e-a299-42e2-fa19-51003b9a8955"
      },
      "source": [
        "# Finding best C for best parameter\n",
        "print(grid1.best_params_)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiVrKUJpfbBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca2e6b2f-1719-4785-a71f-6a8148561d16"
      },
      "source": [
        "# Finding best accuracy\n",
        "print(grid1.best_score_)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9823458282950422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMDfxwAVfenk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "2fb11008-eb86-4d0d-a38a-e8b3880b9733"
      },
      "source": [
        "# Training test dataset\n",
        "grid1.fit(X_train1,y_train1)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None, param_grid={'C': [0.1, 1, 10, 100]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKUntzwifk0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b66d6202-22a8-47a9-bec2-9736f03a2082"
      },
      "source": [
        "# Prediction of test data\n",
        "pred3=grid1.predict(X_test1)\n",
        "pred3"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvT_8ttFfrRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d56f6b3d-f42b-4d99-ff68-241c1e44a0e6"
      },
      "source": [
        "# Evaluate the model and training dataset\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "print(classification_report(y_test1,pred3))\n",
        "print()\n",
        "print('confusion Matrix:\\n',confusion_matrix(y_test1,pred3))\n",
        "print()\n",
        "print('accuracy score:\\n',accuracy_score(y_test1,pred3))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       885\n",
            "        spam       0.99      0.91      0.95       149\n",
            "\n",
            "    accuracy                           0.99      1034\n",
            "   macro avg       0.99      0.95      0.97      1034\n",
            "weighted avg       0.99      0.99      0.99      1034\n",
            "\n",
            "\n",
            "confusion Matrix:\n",
            " [[884   1]\n",
            " [ 14 135]]\n",
            "\n",
            "accuracy score:\n",
            " 0.9854932301740812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diIh_kAufyqz",
        "colab_type": "text"
      },
      "source": [
        "As we compare the Results we get an better Accuracy score on testing (98.54%) with Count Vectorizer. It is acceptable Accuracy score as we get better f-1 score as well, it is an indication that our model classification accuracy is good with better precision & recall Score.\n",
        "\n",
        "All these parameters are important while predicting results from classification report which will help us in decision making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB7LyofKgkiG",
        "colab_type": "text"
      },
      "source": [
        "HAPPY LEARNING !!!!\n",
        "\n",
        "STAY TUNED WITH MY BLOGS @ https://www.nileshgode.info/blog"
      ]
    }
  ]
}