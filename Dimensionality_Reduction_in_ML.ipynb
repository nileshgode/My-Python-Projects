{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality Reduction in ML",
      "provenance": [],
      "authorship_tag": "ABX9TyPzxE+wsr3qkJAPikgNG+8r"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXS0V-ox3uSu",
        "colab_type": "text"
      },
      "source": [
        "By the end of this chapter, you will be able to analyze datasets with high dimensions and deal with the challenges posed by these datasets. As well as applying different dimensionality reduction techniques to large datasets, you will fit models based on those datasets and analyze their results.\n",
        "\n",
        "when the number of variables you have to deal with is large, say around 18 million instead of the 18 you dealt with in the last chapter? How do you load such large datasets and analyze them? How do you deal with the computing resources required for modeling with such large datasets?\n",
        "\n",
        "Healthcare, where genetics datasets can have millions of features\n",
        "High-resolution imaging datasets\n",
        "Web data related to advertisements, ranking, and crawling\n",
        "\n",
        "When dealing with such huge datasets, many challenges can arise:\n",
        "\n",
        "Storage and computation challenges: Large datasets with high dimensions require a lot of storage and expensive computational resources for analysis.\n",
        "\n",
        "Exploration challenges: When trying to explore data and derive insights, high-dimensional data can be really cumbersome.\n",
        "\n",
        "Algorithm challenges: Many algorithms do not scale well in high-dimensional settings.\n",
        "\n",
        "Dimensionality reduction aims to reduce the dimensions of datasets to get over the challenges posed by high-dimensional data. In this chapter, we will examine some of the popular dimensionality reduction techniques:\n",
        "\n",
        "Backward feature elimination or recursive feature elimination\n",
        "Forward feature selection\n",
        "Principal Component Analysis (PCA)\n",
        "Independent Component Analysis (ICA)\n",
        "Factor analysis\n",
        "\n",
        "**Let's first examine our business context and then apply these techniques to the problem statement.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM1O-7ik4geW",
        "colab_type": "text"
      },
      "source": [
        "***Business Context***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5tzbZ6V5b7x",
        "colab_type": "text"
      },
      "source": [
        "The marketing head of your company comes to you with a problem he has been grappling with. Many customers have been complaining about the browsing experience of your company's website because of the number of advertisements that pop up during browsing. Your company wants to build an engine on your web server that identifies potential advertisements and then eliminates them even before they pop up.\n",
        "\n",
        "To help you to achieve this, you have been given a dataset that contains a set of possible advertisements on a variety of web pages. The features of the dataset represent the geometry of the images in the possible adverts, as well as phrases occurring in the URL, image URLs, anchor text, and words occurring near the anchor text. This dataset has also been labeled, with each possible ad given a label that says whether it is actually an advertisement or not. **Using this dataset, you have to build a model that predicts whether something is an advertisement or not.** You may think that this is a relatively simple problem that could be solved with any binary classification algorithm. However, there is a challenge in the dataset. **The dataset has a large number of features. You have set out to solve this high-dimensional dataset challenge.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fm55rg75-1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfo_fR3lws_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining file name of the GitHub repository\n",
        "filename = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter14/Dataset/ad.data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_slYhHNS6W6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "897832d6-40d2-4d1a-d961-fe38d256caee"
      },
      "source": [
        "adData = pd.read_csv(filename,sep=\",\",header = None,error_bad_lines=False)\n",
        "adData.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1519</th>\n",
              "      <th>1520</th>\n",
              "      <th>1521</th>\n",
              "      <th>1522</th>\n",
              "      <th>1523</th>\n",
              "      <th>1524</th>\n",
              "      <th>1525</th>\n",
              "      <th>1526</th>\n",
              "      <th>1527</th>\n",
              "      <th>1528</th>\n",
              "      <th>1529</th>\n",
              "      <th>1530</th>\n",
              "      <th>1531</th>\n",
              "      <th>1532</th>\n",
              "      <th>1533</th>\n",
              "      <th>1534</th>\n",
              "      <th>1535</th>\n",
              "      <th>1536</th>\n",
              "      <th>1537</th>\n",
              "      <th>1538</th>\n",
              "      <th>1539</th>\n",
              "      <th>1540</th>\n",
              "      <th>1541</th>\n",
              "      <th>1542</th>\n",
              "      <th>1543</th>\n",
              "      <th>1544</th>\n",
              "      <th>1545</th>\n",
              "      <th>1546</th>\n",
              "      <th>1547</th>\n",
              "      <th>1548</th>\n",
              "      <th>1549</th>\n",
              "      <th>1550</th>\n",
              "      <th>1551</th>\n",
              "      <th>1552</th>\n",
              "      <th>1553</th>\n",
              "      <th>1554</th>\n",
              "      <th>1555</th>\n",
              "      <th>1556</th>\n",
              "      <th>1557</th>\n",
              "      <th>1558</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ad.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>468</td>\n",
              "      <td>8.2105</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ad.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>230</td>\n",
              "      <td>6.9696</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ad.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ad.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ad.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 1559 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1       2    3     4     5     ...  1553  1554  1555  1556  1557  1558\n",
              "0   125   125     1.0    1     0     0  ...     0     0     0     0     0   ad.\n",
              "1    57   468  8.2105    1     0     0  ...     0     0     0     0     0   ad.\n",
              "2    33   230  6.9696    1     0     0  ...     0     0     0     0     0   ad.\n",
              "3    60   468     7.8    1     0     0  ...     0     0     0     0     0   ad.\n",
              "4    60   468     7.8    1     0     0  ...     0     0     0     0     0   ad.\n",
              "\n",
              "[5 rows x 1559 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfQ8-yFu6rOI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6be7026-6750-4e6e-94cb-49356f03826f"
      },
      "source": [
        "# Printing the shape of the data\n",
        "print(adData.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3279, 1559)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFvkCKPo6u4j",
        "colab_type": "text"
      },
      "source": [
        "From the shape, we can see that we have a large number of features, 1559.\n",
        "\n",
        "Find the summary of the numerical features of the raw data using the .describe() function in pandas, as shown in the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wavcFXes6spy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "c004e4e8-5963-4e62-d965-9c09ab643279"
      },
      "source": [
        "# Summarizing the statistics of the numerical raw data\n",
        "adData.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>...</th>\n",
              "      <th>1518</th>\n",
              "      <th>1519</th>\n",
              "      <th>1520</th>\n",
              "      <th>1521</th>\n",
              "      <th>1522</th>\n",
              "      <th>1523</th>\n",
              "      <th>1524</th>\n",
              "      <th>1525</th>\n",
              "      <th>1526</th>\n",
              "      <th>1527</th>\n",
              "      <th>1528</th>\n",
              "      <th>1529</th>\n",
              "      <th>1530</th>\n",
              "      <th>1531</th>\n",
              "      <th>1532</th>\n",
              "      <th>1533</th>\n",
              "      <th>1534</th>\n",
              "      <th>1535</th>\n",
              "      <th>1536</th>\n",
              "      <th>1537</th>\n",
              "      <th>1538</th>\n",
              "      <th>1539</th>\n",
              "      <th>1540</th>\n",
              "      <th>1541</th>\n",
              "      <th>1542</th>\n",
              "      <th>1543</th>\n",
              "      <th>1544</th>\n",
              "      <th>1545</th>\n",
              "      <th>1546</th>\n",
              "      <th>1547</th>\n",
              "      <th>1548</th>\n",
              "      <th>1549</th>\n",
              "      <th>1550</th>\n",
              "      <th>1551</th>\n",
              "      <th>1552</th>\n",
              "      <th>1553</th>\n",
              "      <th>1554</th>\n",
              "      <th>1555</th>\n",
              "      <th>1556</th>\n",
              "      <th>1557</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "      <td>3279.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.004270</td>\n",
              "      <td>0.011589</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.003965</td>\n",
              "      <td>0.011589</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.009149</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.003965</td>\n",
              "      <td>0.005489</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.004270</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.006099</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.004270</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.011894</td>\n",
              "      <td>0.009149</td>\n",
              "      <td>0.017078</td>\n",
              "      <td>0.005185</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.009759</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.004270</td>\n",
              "      <td>0.008539</td>\n",
              "      <td>0.005794</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018298</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.005489</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.016773</td>\n",
              "      <td>0.004880</td>\n",
              "      <td>0.005794</td>\n",
              "      <td>0.005794</td>\n",
              "      <td>0.010674</td>\n",
              "      <td>0.011894</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.014639</td>\n",
              "      <td>0.004270</td>\n",
              "      <td>0.005794</td>\n",
              "      <td>0.027447</td>\n",
              "      <td>0.010979</td>\n",
              "      <td>0.003355</td>\n",
              "      <td>0.005185</td>\n",
              "      <td>0.005185</td>\n",
              "      <td>0.015249</td>\n",
              "      <td>0.007014</td>\n",
              "      <td>0.007319</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.012504</td>\n",
              "      <td>0.016163</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.005185</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.005489</td>\n",
              "      <td>0.009759</td>\n",
              "      <td>0.006099</td>\n",
              "      <td>0.004575</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>0.002440</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.006404</td>\n",
              "      <td>0.012809</td>\n",
              "      <td>0.013419</td>\n",
              "      <td>0.009759</td>\n",
              "      <td>0.001525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.065212</td>\n",
              "      <td>0.107042</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.057831</td>\n",
              "      <td>0.062850</td>\n",
              "      <td>0.107042</td>\n",
              "      <td>0.057831</td>\n",
              "      <td>0.069694</td>\n",
              "      <td>0.095227</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.069694</td>\n",
              "      <td>0.062850</td>\n",
              "      <td>0.073899</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.069694</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.055148</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.065212</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.077872</td>\n",
              "      <td>0.057831</td>\n",
              "      <td>0.065212</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.108425</td>\n",
              "      <td>0.095227</td>\n",
              "      <td>0.129583</td>\n",
              "      <td>0.071828</td>\n",
              "      <td>0.055148</td>\n",
              "      <td>0.057831</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.098320</td>\n",
              "      <td>0.057831</td>\n",
              "      <td>0.065212</td>\n",
              "      <td>0.092026</td>\n",
              "      <td>0.075912</td>\n",
              "      <td>...</td>\n",
              "      <td>0.134048</td>\n",
              "      <td>0.069694</td>\n",
              "      <td>0.073899</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.128441</td>\n",
              "      <td>0.069694</td>\n",
              "      <td>0.075912</td>\n",
              "      <td>0.075912</td>\n",
              "      <td>0.102778</td>\n",
              "      <td>0.108425</td>\n",
              "      <td>0.055148</td>\n",
              "      <td>0.120120</td>\n",
              "      <td>0.065212</td>\n",
              "      <td>0.075912</td>\n",
              "      <td>0.163408</td>\n",
              "      <td>0.104220</td>\n",
              "      <td>0.057831</td>\n",
              "      <td>0.071828</td>\n",
              "      <td>0.071828</td>\n",
              "      <td>0.122559</td>\n",
              "      <td>0.083470</td>\n",
              "      <td>0.085252</td>\n",
              "      <td>0.055148</td>\n",
              "      <td>0.111136</td>\n",
              "      <td>0.126123</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.071828</td>\n",
              "      <td>0.055148</td>\n",
              "      <td>0.073899</td>\n",
              "      <td>0.098320</td>\n",
              "      <td>0.077872</td>\n",
              "      <td>0.067491</td>\n",
              "      <td>0.060393</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.055148</td>\n",
              "      <td>0.079783</td>\n",
              "      <td>0.112466</td>\n",
              "      <td>0.115077</td>\n",
              "      <td>0.098320</td>\n",
              "      <td>0.039026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 1554 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              4            5     ...         1556         1557\n",
              "count  3279.000000  3279.000000  ...  3279.000000  3279.000000\n",
              "mean      0.004270     0.011589  ...     0.009759     0.001525\n",
              "std       0.065212     0.107042  ...     0.098320     0.039026\n",
              "min       0.000000     0.000000  ...     0.000000     0.000000\n",
              "25%       0.000000     0.000000  ...     0.000000     0.000000\n",
              "50%       0.000000     0.000000  ...     0.000000     0.000000\n",
              "75%       0.000000     0.000000  ...     0.000000     0.000000\n",
              "max       1.000000     1.000000  ...     1.000000     1.000000\n",
              "\n",
              "[8 rows x 1554 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGyXLv9x7ODP",
        "colab_type": "text"
      },
      "source": [
        "Separate the dependent and independent variables from our dataset, as shown in the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abWXO2RE7Ola",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d596fe0-6adb-4181-e805-aa3107e21510"
      },
      "source": [
        "# Separate the dependent and independent variables\n",
        "# Preparing the X variables\n",
        "X = adData.loc[:,0:1557]\n",
        "print(X.shape)\n",
        "# Preparing the Y variable\n",
        "Y = adData[1558]\n",
        "print(Y.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3279, 1558)\n",
            "(3279,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbdzga4x8xOV",
        "colab_type": "text"
      },
      "source": [
        "Print the first 15 examples of the independent variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUYRbCUl8xzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "c9daba21-9d3e-46a4-fcc1-929809771f18"
      },
      "source": [
        "# Printing the head of the independent variables\n",
        "X.head(15)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1518</th>\n",
              "      <th>1519</th>\n",
              "      <th>1520</th>\n",
              "      <th>1521</th>\n",
              "      <th>1522</th>\n",
              "      <th>1523</th>\n",
              "      <th>1524</th>\n",
              "      <th>1525</th>\n",
              "      <th>1526</th>\n",
              "      <th>1527</th>\n",
              "      <th>1528</th>\n",
              "      <th>1529</th>\n",
              "      <th>1530</th>\n",
              "      <th>1531</th>\n",
              "      <th>1532</th>\n",
              "      <th>1533</th>\n",
              "      <th>1534</th>\n",
              "      <th>1535</th>\n",
              "      <th>1536</th>\n",
              "      <th>1537</th>\n",
              "      <th>1538</th>\n",
              "      <th>1539</th>\n",
              "      <th>1540</th>\n",
              "      <th>1541</th>\n",
              "      <th>1542</th>\n",
              "      <th>1543</th>\n",
              "      <th>1544</th>\n",
              "      <th>1545</th>\n",
              "      <th>1546</th>\n",
              "      <th>1547</th>\n",
              "      <th>1548</th>\n",
              "      <th>1549</th>\n",
              "      <th>1550</th>\n",
              "      <th>1551</th>\n",
              "      <th>1552</th>\n",
              "      <th>1553</th>\n",
              "      <th>1554</th>\n",
              "      <th>1555</th>\n",
              "      <th>1556</th>\n",
              "      <th>1557</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>468</td>\n",
              "      <td>8.2105</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>230</td>\n",
              "      <td>6.9696</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>59</td>\n",
              "      <td>460</td>\n",
              "      <td>7.7966</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>60</td>\n",
              "      <td>234</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>60</td>\n",
              "      <td>468</td>\n",
              "      <td>7.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>90</td>\n",
              "      <td>52</td>\n",
              "      <td>0.5777</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>90</td>\n",
              "      <td>60</td>\n",
              "      <td>0.6666</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>90</td>\n",
              "      <td>60</td>\n",
              "      <td>0.6666</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>33</td>\n",
              "      <td>230</td>\n",
              "      <td>6.9696</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15 rows Ã— 1558 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    0     1       2    3     4     5     ...  1552  1553  1554  1555  1556  1557\n",
              "0    125   125     1.0    1     0     0  ...     0     0     0     0     0     0\n",
              "1     57   468  8.2105    1     0     0  ...     0     0     0     0     0     0\n",
              "2     33   230  6.9696    1     0     0  ...     0     0     0     0     0     0\n",
              "3     60   468     7.8    1     0     0  ...     0     0     0     0     0     0\n",
              "4     60   468     7.8    1     0     0  ...     0     0     0     0     0     0\n",
              "5     60   468     7.8    1     0     0  ...     0     0     0     0     0     0\n",
              "6     59   460  7.7966    1     0     0  ...     0     0     0     0     0     0\n",
              "7     60   234     3.9    1     0     0  ...     0     0     0     0     0     0\n",
              "8     60   468     7.8    1     0     0  ...     0     0     0     0     0     0\n",
              "9     60   468     7.8    1     0     0  ...     0     0     0     0     0     0\n",
              "10     ?     ?       ?    1     0     0  ...     0     0     0     0     0     0\n",
              "11    90    52  0.5777    1     0     0  ...     0     0     0     0     0     0\n",
              "12    90    60  0.6666    1     0     0  ...     0     0     0     0     0     0\n",
              "13    90    60  0.6666    1     0     0  ...     0     0     0     0     0     0\n",
              "14    33   230  6.9696    1     0     0  ...     0     0     0     0     0     0\n",
              "\n",
              "[15 rows x 1558 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28OiKyxs879j",
        "colab_type": "text"
      },
      "source": [
        "From the output, we can see that there are many missing values in the dataset, which are represented by ?. For further analysis, we have to remove these special characters and then replace those cells with assumed values. One popular method of replacing special characters is to impute the mean of the respective feature. Let's adopt this strategy. However, before doing that, let's look at the data types for this dataset to adopt a suitable replacement strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAIeX2AG88fI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a4f9dd13-bbff-42a2-c05b-431061b2ff30"
      },
      "source": [
        "# Printing the data types\n",
        "print(X.dtypes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       object\n",
            "1       object\n",
            "2       object\n",
            "3       object\n",
            "4        int64\n",
            "         ...  \n",
            "1553     int64\n",
            "1554     int64\n",
            "1555     int64\n",
            "1556     int64\n",
            "1557     int64\n",
            "Length: 1558, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ED_t1Ly9HnW",
        "colab_type": "text"
      },
      "source": [
        "From the output, we can see that the first four columns are of the object type, which refers to string data, and the others are integer data. When replacing the special characters in the data, we need to be cognizant of the data types.\n",
        "\n",
        "\" Replacing special characters with NaN values makes it easy to further impute data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc24EGp09WKm",
        "colab_type": "text"
      },
      "source": [
        "To replace the first three columns, we loop through the columns using the for() loop and also using the range() function. Since the first three columns are of the object or string type, we use the .str.replace() function, which stands for \"string replace\". After replacing the special characters, ?, of the data with nan, we convert the data type to float with the .values.astype(float) function, which is required for further processing. By printing the first 15 examples, we can see that all special characters have been replaced with nan or NaN values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOn_WBkAArCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a2496821-9e23-4d62-d13f-939e60c42d85"
      },
      "source": [
        "# Replacing special characters in first 3 columns which are of type object\n",
        "for i in range(0,3):\n",
        "      X[i] = X[i].str.replace(\"?\", 'nan').values.astype(float)\n",
        "print(X.head(15))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0      1       2    3     4     5     ...  1552  1553  1554  1555  1556  1557\n",
            "0   125.0  125.0  1.0000    1     0     0  ...     0     0     0     0     0     0\n",
            "1    57.0  468.0  8.2105    1     0     0  ...     0     0     0     0     0     0\n",
            "2    33.0  230.0  6.9696    1     0     0  ...     0     0     0     0     0     0\n",
            "3    60.0  468.0  7.8000    1     0     0  ...     0     0     0     0     0     0\n",
            "4    60.0  468.0  7.8000    1     0     0  ...     0     0     0     0     0     0\n",
            "5    60.0  468.0  7.8000    1     0     0  ...     0     0     0     0     0     0\n",
            "6    59.0  460.0  7.7966    1     0     0  ...     0     0     0     0     0     0\n",
            "7    60.0  234.0  3.9000    1     0     0  ...     0     0     0     0     0     0\n",
            "8    60.0  468.0  7.8000    1     0     0  ...     0     0     0     0     0     0\n",
            "9    60.0  468.0  7.8000    1     0     0  ...     0     0     0     0     0     0\n",
            "10    NaN    NaN     NaN    1     0     0  ...     0     0     0     0     0     0\n",
            "11   90.0   52.0  0.5777    1     0     0  ...     0     0     0     0     0     0\n",
            "12   90.0   60.0  0.6666    1     0     0  ...     0     0     0     0     0     0\n",
            "13   90.0   60.0  0.6666    1     0     0  ...     0     0     0     0     0     0\n",
            "14   33.0  230.0  6.9696    1     0     0  ...     0     0     0     0     0     0\n",
            "\n",
            "[15 rows x 1558 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lngVLv-nBPUi",
        "colab_type": "text"
      },
      "source": [
        "o replace the first three columns, we loop through the columns using the for() loop and also using the range() function. Since the first three columns are of the object or string type, we use the .str.replace() function, which stands for \"string replace\". After replacing the special characters, ?, of the data with nan, we convert the data type to float with the .values.astype(float) function, which is required for further processing. By printing the first 15 examples, we can see that all special characters have been replaced with nan or NaN values\n",
        "\n",
        "\n",
        "Now, replace special characters for the integer features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSqh4BG6BV0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replacing special characters in the remaining columns which are of type integer\n",
        "for i in range(3,1557):\n",
        "  X[i] = X[i].replace(\"?\", 'NaN').values.astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKp5SXKrBdAG",
        "colab_type": "text"
      },
      "source": [
        "Now that we have replaced special characters in the data with NaN values, we can use the fillna() function in pandas to replace the NaN values with the mean of the column. This is executed using the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbKh8OF-BdfW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ba824370-ad30-4ff0-c196-ebb404c5511e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Impute the 'NaN' with mean of the values\n",
        "for i in range(0,1557):\n",
        "  X[i] = X[i].fillna(X[i].mean())\n",
        "print(X.head(15))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          0           1         2     3     4     ...  1553  1554  1555  1556  1557\n",
            "0   125.000000  125.000000  1.000000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "1    57.000000  468.000000  8.210500   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "2    33.000000  230.000000  6.969600   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "3    60.000000  468.000000  7.800000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "4    60.000000  468.000000  7.800000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "5    60.000000  468.000000  7.800000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "6    59.000000  460.000000  7.796600   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "7    60.000000  234.000000  3.900000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "8    60.000000  468.000000  7.800000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "9    60.000000  468.000000  7.800000   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "10   64.021886  155.344828  3.911953   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "11   90.000000   52.000000  0.577700   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "12   90.000000   60.000000  0.666600   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "13   90.000000   60.000000  0.666600   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "14   33.000000  230.000000  6.969600   1.0   0.0  ...   0.0   0.0   0.0   0.0     0\n",
            "\n",
            "[15 rows x 1558 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrN7UxgPBlkh",
        "colab_type": "text"
      },
      "source": [
        "Scale the dataset using the minmaxScaler() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJEkheZ4BmEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "85b8c13e-cd32-4648-a7e4-5e707c7f1221"
      },
      "source": [
        "# Scaling the data sets\n",
        "# Import library function\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Creating the scaling function\n",
        "minmaxScaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Transforming with the scaler function\n",
        "X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))\n",
        "\n",
        "# Printing the output\n",
        "X_tran.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1518</th>\n",
              "      <th>1519</th>\n",
              "      <th>1520</th>\n",
              "      <th>1521</th>\n",
              "      <th>1522</th>\n",
              "      <th>1523</th>\n",
              "      <th>1524</th>\n",
              "      <th>1525</th>\n",
              "      <th>1526</th>\n",
              "      <th>1527</th>\n",
              "      <th>1528</th>\n",
              "      <th>1529</th>\n",
              "      <th>1530</th>\n",
              "      <th>1531</th>\n",
              "      <th>1532</th>\n",
              "      <th>1533</th>\n",
              "      <th>1534</th>\n",
              "      <th>1535</th>\n",
              "      <th>1536</th>\n",
              "      <th>1537</th>\n",
              "      <th>1538</th>\n",
              "      <th>1539</th>\n",
              "      <th>1540</th>\n",
              "      <th>1541</th>\n",
              "      <th>1542</th>\n",
              "      <th>1543</th>\n",
              "      <th>1544</th>\n",
              "      <th>1545</th>\n",
              "      <th>1546</th>\n",
              "      <th>1547</th>\n",
              "      <th>1548</th>\n",
              "      <th>1549</th>\n",
              "      <th>1550</th>\n",
              "      <th>1551</th>\n",
              "      <th>1552</th>\n",
              "      <th>1553</th>\n",
              "      <th>1554</th>\n",
              "      <th>1555</th>\n",
              "      <th>1556</th>\n",
              "      <th>1557</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.194053</td>\n",
              "      <td>0.194053</td>\n",
              "      <td>0.016642</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.087637</td>\n",
              "      <td>0.730829</td>\n",
              "      <td>0.136820</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.050078</td>\n",
              "      <td>0.358372</td>\n",
              "      <td>0.116138</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.092332</td>\n",
              "      <td>0.730829</td>\n",
              "      <td>0.129978</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.092332</td>\n",
              "      <td>0.730829</td>\n",
              "      <td>0.129978</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 1558 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2     3     4     ...  1553  1554  1555  1556  1557\n",
              "0  0.194053  0.194053  0.016642   1.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "1  0.087637  0.730829  0.136820   1.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "2  0.050078  0.358372  0.116138   1.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "3  0.092332  0.730829  0.129978   1.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "4  0.092332  0.730829  0.129978   1.0   0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 1558 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0MJ7N2vBtgN",
        "colab_type": "text"
      },
      "source": [
        "You have come to the end of the first exercise. In this exercise, we loaded the dataset, extracted summary statistics, cleaned the data, and also scaled the data. You can see that in the final output, all the raw values have been transformed into scaled values.\n",
        "\n",
        ". In order to demonstrate the challenges with high-dimensional datasets, let's create an extremely high dimensional dataset from the internet dataset that we already have.\n",
        "\n",
        "This we will achieve by replicating the existing number of features multiple times so that the dataset becomes really large. To replicate the dataset, we will use a function called np.tile(), which copies a data frame multiple times across the axes we want. We will also calculate the time it takes for any activity using the time() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2JYnrB5CsEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_brb5MaC_iX",
        "colab_type": "text"
      },
      "source": [
        "Then, to create a dummy data frame, we will use a small dataset with two rows and three columns for this example. We use the pd.np.array() function to create a data frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk-MxSvDDB4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8e919cd6-65d2-4f03-f2d9-2aced92f0b0c"
      },
      "source": [
        "# Creating a simple data frame\n",
        "df = pd.np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(df.shape)\n",
        "df"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBFnAoz3DQjG",
        "colab_type": "text"
      },
      "source": [
        "Next, you replicate the dummy data frame and this replication of the columns is done using the pd.np.tile() function in the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk17wgxFDU_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "64330443-47c3-4895-9682-a91b67c803df"
      },
      "source": [
        "# # Replicating the data frame and noting the time\n",
        "import time\n",
        "# Starting a timing function\n",
        "t0=time.time()\n",
        "Newdf = pd.DataFrame(pd.np.tile(df, (1, 5)))\n",
        "print(Newdf.shape)\n",
        "print(Newdf)\n",
        "# Finding the end time \n",
        "print(\"Total time:\", round(time.time()-t0, 3), \"s\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 15)\n",
            "   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14\n",
            "0  1  2  3  1  2  3  1  2  3  1   2   3   1   2   3\n",
            "1  4  5  6  4  5  6  4  5  6  4   5   6   4   5   6\n",
            "Total time: 0.02 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x91Lb23BDjSV",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the snippet, the pd.np.tile() function accepts two sets of arguments. The first one is the data frame, df, that we want to replicate. The next argument, (1,5), defines which axes we want to replicate. In this example, we define that the rows will remain as is because of the 1 argument, and the columns will be replicated 5 times with the 5 argument. We can see from the shape() function that the original data frame, which was of shape (2,3), has been transformed into a data frame with a shape of (2,15).\n",
        "\n",
        "Calculating the total time is done using the time library. To start the timing, we invoke the time.time() function. In the example, we store the initial time in a variable called t0 and then subtract this from the end time to find the total time it takes for the process. Thus we have augmented and added more data frames to our exiting internet ads dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtnSpvegGyoW",
        "colab_type": "text"
      },
      "source": [
        "*Because of resource limitations we can not run some methods on colab, methods are a compute-intensive processes, and therefore this processes will take a lot of time to execute. The larger the number of features, the longer it will take. so we will only discuss steps as follows:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fdEfHlODkJK",
        "colab_type": "text"
      },
      "source": [
        "**Backward Feature Elimination (Recursive Feature Elimination)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgDLLLEaFhwe",
        "colab_type": "text"
      },
      "source": [
        "The mechanism behind the backward feature elimination algorithm is the recursive elimination of features and building a model on those features that remain after all the elimination.\n",
        "\n",
        "Let's look under the hood of this algorithm step by step\n",
        "\n",
        "1.   Initially, at a given iteration, the selected classification algorithm is first trained on all the n features available. For example, let's take the case of the original dataset we had, which had 1,558 features. The algorithm starts off with all the 1,558 features in the first iteration.\n",
        "2.  In the next step, we remove one feature at a time and train a model with the remaining n-1 features. This process is repeated n times. For example, we first remove feature 1 and then fit a model using all the remaining 1,557 variables. In the next iteration, we use feature 1 and instead, we eliminate feature 2 and then fit the model. This process is repeated n times (1,558) times.\n",
        "3.   For each of the models fitted, the performance of the model (using measures such as accuracy) is calculated.\n",
        "4.   The feature whose replacement has resulted in the smallest change in performance is removed permanently and Step 2 is repeated with n-1 features.\n",
        "5.   The process is then repeated with n-2 features and so on.\n",
        "6.   The algorithm keeps on eliminating features until the threshold number of features we require is reached.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao7I6PQ-GT32",
        "colab_type": "text"
      },
      "source": [
        "**Dimensionality Reduction Using Backward Feature Elimination**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEVF7FNwGats",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to fit the backward elimination method on the higher-dimensional dataset. We will also note the time it takes for backward elimination to work. This is implemented using the following code snippet:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-IGv9Z_d8NT",
        "colab_type": "text"
      },
      "source": [
        "**Principal Component Analysis (PCA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3lTM11_d-ls",
        "colab_type": "text"
      },
      "source": [
        "The basic idea behind PCA is to first identify correlations among different variables within the dataset. Once correlations are identified, the algorithm decides to eliminate the variables in such a way that the variability of the data is maintained. In other words, PCA aims to find uncorrelated sources of data.\n",
        "\n",
        "Implementing PCA on raw variables results in transforming them into a completely new set of variables called principal components. Each of these components represents variability in data along an axes that are orthogonal to each other. This means that the first axis is fit in the direction where the maximum variability of data is present. After this, the second axis is selected in such a way that the axis is orthogonal (perpendicular) to the first selected axis and also covers the next highest variability.\n",
        "\n",
        "Let's look at the idea of PCA with an example.\n",
        "\n",
        "We will create a sample dataset with 2 variables and 100 random data points in each variable. Random data points are created using the rand() function. This is implemented in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMA9JOOue8T8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e30244b-cc7f-4ddf-be72-8e09293c5fd6"
      },
      "source": [
        "import numpy as np\n",
        "# Setting the seed for reproducibility\n",
        "seed = np.random.RandomState(123)\n",
        "# Generating an array of random numbers\n",
        "X = seed.rand(100,2)\n",
        "# Printing the shape of the dataset\n",
        "X.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-piQiEifQmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "638b0bab-d78b-4259-d585-111d4529b189"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.04635361265714105,\n",
              " 1.0325632864350172,\n",
              " -0.003996887112708299,\n",
              " 1.0429468329457663)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auq_PYy4fT1y",
        "colab_type": "text"
      },
      "source": [
        "In the graph, we can see that the data is evenly spread out.\n",
        "\n",
        "Let's now find the principal components for this dataset. We will reduce this two-dimensional dataset into a one-dimensional dataset. In other words, we will reduce the original dataset into one of its principal components.\n",
        "\n",
        "This is implemented in code as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyYrjBXZfXwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "830d9df0-cee8-49b1-9589-0d9e006d4a02"
      },
      "source": [
        "from sklearn.decomposition import PCA \n",
        "\n",
        "# Defining one component \n",
        "pca = PCA(n_components=1) \n",
        "\n",
        "# Fitting the PCA function \n",
        "pca.fit(X) \n",
        "\n",
        "# Getting the new dataset \n",
        "X_pca = pca.transform(X) \n",
        "\n",
        "# Printing the shapes \n",
        "print(\"Original data set: \", X.shape) \n",
        "print(\"Data set after transformation:\", X_pca.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original data set:  (100, 2)\n",
            "Data set after transformation: (100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kPr8IPRftVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcY92nsPfy5L",
        "colab_type": "text"
      },
      "source": [
        "The algorithm transforms the original dataset into its first principal component by using an axis where the data has the largest variability.\n",
        "\n",
        "To visualize this concept, let's reverse the transformation of the X_pca dataset to its original form and then visualize this data along with the original data. To reverse the transformation, we use the .inverse_transform() function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLgSjD0Kf373",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "85e21b2d-f434-4680-e989-fe46bf6905cb"
      },
      "source": [
        "# Reversing the transformation and plotting\n",
        "X_reverse = pca.inverse_transform(X_pca)\n",
        "\n",
        "# Plotting the original data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.1)\n",
        "\n",
        "# Plotting the reversed data\n",
        "plt.scatter(X_reverse[:, 0], X_reverse[:, 1], alpha=0.9)\n",
        "plt.axis('equal');"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U0Z3atNgHcd",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the plot, the data points in orange represent an axis with the highest variability. All the data points were projected to that axis to generate the first principal component.\n",
        "\n",
        "The data points that are generated when transforming into various principal components will be very different from the original data points before transformation. Each principal component will be in an axis that is orthogonal (perpendicular) to the other principal component. If a second principal component was generated for the preceding example, the second principal component would be along an axis indicated by the blue arrow in the graph. The way we pick the number of principal components for model building is by selecting the number of components that explains a certain threshold of variability.\n",
        "\n",
        "For example, if there were originally 1,000 features and we reduced it to 100 principal components, and then we find that out of the 100 principal components the first 75 components explain 90% of the variability of data, we would pick those 75 components to build the model. This process is called picking principal components with the percentage of variance explained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQgPxG2gYsF",
        "colab_type": "text"
      },
      "source": [
        "we will fit a logistic regression model by selecting the principal components that explain the maximum variability of the data. We will also observe the performance of the feature selection and model building process. We will be using the same ads dataset as before, and we will be enhancing it with additional features for this exercise. we are continuing with same data set which was exported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqTDEOIwg08P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0d8facc8-1b53-48a1-99ae-bdfa1666a52a"
      },
      "source": [
        "# Creating a high dimension data set\n",
        "X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))\n",
        "print(X_hd.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(3279, 77900)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlzO3uSQg5en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T91dUimPhLfU",
        "colab_type": "text"
      },
      "source": [
        "Let's now fit the PCA function on the training set. This is done using the .fit() function, as shown in the following snippet. We will also note the time it takes to fit the PCA model on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ow522QzhOBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5bc5bc9-8cd1-4720-e10a-ede449480f5d"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "t0 = time.time()\n",
        "pca = PCA().fit(X_train)\n",
        "t1 = time.time()\n",
        "print(\"PCA fitting time:\", round(t1-t0, 3), \"s\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA fitting time: 169.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtyRK8SrhY5T",
        "colab_type": "text"
      },
      "source": [
        "One can easily observe the 'RAM' uses exceed 9GB which is not possible to simulate results on normal laptops or systems, it require high end machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9T7iGgch5Wg",
        "colab_type": "text"
      },
      "source": [
        "We will now determine the number of principal components by plotting the cumulative variance explained by all the principal components. The variance explained is determined by the pca.explained_variance_ratio_ method. This is plotted in matplotlib using the following code snippet:\n",
        "In the code, the np.cumsum() function is used to get the cumulative variance of each principal component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwn5c9s1h6KE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "aab4bbd1-48fd-4fc2-bed5-91e6077f784a"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative explained variance');"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RAsT812iNJd",
        "colab_type": "text"
      },
      "source": [
        "From the plot, we can see that the first 250 principal components explain more than 90% of the variance. Based on this graph, we can decide how many principal components we want to have depending on the variability it explains. Let's select 250 components for fitting our model.\n",
        "\n",
        "Now that we have identified that 250 components explain a lot of the variability, let's refit the training set for 250 components. This is described in the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bnez8-8iTts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b848a50f-51ba-479d-cdcf-07e4a396eb2d"
      },
      "source": [
        "# Defining PCA with 250 components\n",
        "pca = PCA(n_components=250)\n",
        "\n",
        "# Fitting PCA on the training set\n",
        "pca.fit(X_train)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=250, random_state=None,\n",
              "    svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhUArFH_iZOD",
        "colab_type": "text"
      },
      "source": [
        "We now transform the training and test sets with the 200 principal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByJ14O_3iZxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming training set and test set\n",
        "\n",
        "X_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_gKzVD2ihn5",
        "colab_type": "text"
      },
      "source": [
        "Let's verify the shapes of the train and test sets before transformation and after transformation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM89WAesiiZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bcc35faf-4e20-4aec-98a5-08691f26e15c"
      },
      "source": [
        "# Printing the shape of train and test sets before and after transformation\n",
        "print(\"original shape of Training set: \", X_train.shape)\n",
        "print(\"original shape of Test set: \", X_test.shape)\n",
        "print(\"Transformed shape of training set:\", X_pca.shape)\n",
        "print(\"Transformed shape of test set:\", X_test_pca.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original shape of Training set:  (2295, 77900)\n",
            "original shape of Test set:  (984, 77900)\n",
            "Transformed shape of training set: (2295, 250)\n",
            "Transformed shape of test set: (984, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNgtYBYizak",
        "colab_type": "text"
      },
      "source": [
        "Let's now fit the logistic regression model on the transformed dataset and note the time it takes to fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzSodT1xiz-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6585af56-00e5-4070-f85e-055fc60bf08d"
      },
      "source": [
        "# Fitting a Logistic Regression Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "pcaModel = LogisticRegression()\n",
        "t0 = time.time()\n",
        "pcaModel.fit(X_pca, y_train)\n",
        "t1 = time.time()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17KXGIMDjKmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6e5e268-2934-4092-f349-668f9ee7d9c8"
      },
      "source": [
        "print(\"Total training time:\", round(t1-t0, 3), \"s\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training time: 0.134 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9vs9wJIjO5D",
        "colab_type": "text"
      },
      "source": [
        "Now, predict on the test set and print the accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxuRw6y0jPbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "595b0514-8c2f-4567-c076-20ad4943a858"
      },
      "source": [
        "# Predicting with the pca model\n",
        "pred = pcaModel.predict(X_test_pca)\n",
        "\n",
        "print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(pcaModel.score(X_test_pca, y_test)))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic regression model prediction on test set: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkArT9OdjZHX",
        "colab_type": "text"
      },
      "source": [
        "You can see that the accuracy level is better than the benchmark model with all the features (97%) and the forward selection model (94%).\n",
        "\n",
        "Print the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6AjHNYYjcD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4885548f-a6f8-41f3-8132-74cc6a636930"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusionMatrix = confusion_matrix(y_test, pred)\n",
        "print(confusionMatrix)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[111  15]\n",
            " [  8 850]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYq0lF_9jiPl",
        "colab_type": "text"
      },
      "source": [
        "Print the classification report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gBlHQd9jiyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "361c5cc3-471f-4e25-e653-f1cd2619d5e8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Getting the Classification_report\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ad.       0.93      0.88      0.91       126\n",
            "      nonad.       0.98      0.99      0.99       858\n",
            "\n",
            "    accuracy                           0.98       984\n",
            "   macro avg       0.96      0.94      0.95       984\n",
            "weighted avg       0.98      0.98      0.98       984\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnendksdjo0I",
        "colab_type": "text"
      },
      "source": [
        "As is evident from the results, we get a score of 98%, which is better than the benchmark model. One reason that could be attributed to the higher performance could be the creation of uncorrelated principal components using the PCA method, which has boosted the performance.\n",
        "\n",
        "One can change the PCA parameters and check the model performance for different values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT71vWM4jqJE",
        "colab_type": "text"
      },
      "source": [
        "**Independent Component Analysis (ICA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RztZsGxrm9eE",
        "colab_type": "text"
      },
      "source": [
        "ICA is a technique of dimensionality reduction that conceptually follows a similar path as PCA. Both ICA and PCA try to derive new sources of data by linearly combining the original data.\n",
        "\n",
        "However, the **difference between them** lies in the method they use to find new sources of data. While **PCA attempts to find uncorrelated sources of data, ICA attempts to find independent sources of data**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iSbd4Q_nQ_M",
        "colab_type": "text"
      },
      "source": [
        "n this exercise, we will fit a logistic regression model using the ICA technique and observe the performance of the model. We will be using the same ads dataset as before, and we will be enhancing it with additional features for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDRaAmwjnsCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "925beaf5-1d65-4945-9da8-95f560588d85"
      },
      "source": [
        "# Creating a high dimension data set\n",
        "X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))\n",
        "print(X_hd.shape)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(3279, 77900)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iqir29vnvbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNy7C9qCnz8f",
        "colab_type": "text"
      },
      "source": [
        "Let's load the ICA function, FastICA, and then define the number of components we require. We will use the same number of components that we used for PCA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMi1jA49nx1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the ICA with number of components\n",
        "from sklearn.decomposition import FastICA\n",
        "ICA = FastICA(n_components=250, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KftwMlg5n8Vk",
        "colab_type": "text"
      },
      "source": [
        "Once the ICA method is defined, we will fit the method on the training set and also transform the training set to get a new training set with the required number of components. We will also note the time taken for fitting and transforming:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daj8n8gxn9B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ced31389-5d31-4a35-eaf1-39b8f157e911"
      },
      "source": [
        "# Fitting the ICA method and transforming the training set\n",
        "import time\n",
        "t0 = time.time()\n",
        "X_ica=ICA.fit_transform(X_train)\n",
        "t1 = time.time()\n",
        "print(\"ICA fitting time:\", round(t1-t0, 3), \"s\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ICA fitting time: 175.674 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMUYGVGoJvR",
        "colab_type": "text"
      },
      "source": [
        "We can see that implementing ICA has taken much more time than PCA (179.54 seconds)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNM8WJgOoN1h",
        "colab_type": "text"
      },
      "source": [
        "We now transform the test set with the 250 components:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGftGu5aoKvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming the test set\n",
        "X_test_ica=ICA.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsnB76FboT4M",
        "colab_type": "text"
      },
      "source": [
        "Let's verify the shapes of the train and test sets before transformation and after transformation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0QFFPxnoWPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1063e5bf-249b-4af2-9cb2-864b7c2d6c7b"
      },
      "source": [
        "# Printing the shape of train and test sets before and after transformation\n",
        "print(\"original shape of Training set: \", X_train.shape)\n",
        "print(\"original shape of Test set: \", X_test.shape)\n",
        "print(\"Transformed shape of training set:\", X_ica.shape)\n",
        "print(\"Transformed shape of test set:\", X_test_ica.shape)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original shape of Training set:  (2295, 77900)\n",
            "original shape of Test set:  (984, 77900)\n",
            "Transformed shape of training set: (2295, 250)\n",
            "Transformed shape of test set: (984, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNq_oGRboYj1",
        "colab_type": "text"
      },
      "source": [
        "Let's now fit the logistic regression model on the transformed dataset and note the time it takes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p9VkWJmoa3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting a Logistic Regression Model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "icaModel = LogisticRegression()\n",
        "t0 = time.time()\n",
        "icaModel.fit(X_ica, y_train)\n",
        "t1 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aww-FcDOodCs",
        "colab_type": "text"
      },
      "source": [
        "Print the total time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7pcvURPof5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "595e5c97-5d5e-4c5b-a94c-4aac39e03670"
      },
      "source": [
        "print(\"Total training time:\", round(t1-t0, 3), \"s\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training time: 0.038 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuhQCwr-omyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c241c40-a108-45f7-85bb-dc967a3ccdcf"
      },
      "source": [
        "# Predicting with the ica model\n",
        "pred = icaModel.predict(X_test_ica)\n",
        "\n",
        "print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(icaModel.score(X_test_ica, y_test)))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic regression model prediction on test set: 0.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7vtThptonb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6616e6ab-3f4c-49cc-8b83-4cd2b5d23568"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusionMatrix = confusion_matrix(y_test, pred)\n",
        "print(confusionMatrix)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0 126]\n",
            " [  0 858]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3knC11SouPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "657c06cf-41e8-4fb4-defd-c7ad46df5b97"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Getting the Classification_report\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ad.       0.00      0.00      0.00       126\n",
            "      nonad.       0.87      1.00      0.93       858\n",
            "\n",
            "    accuracy                           0.87       984\n",
            "   macro avg       0.44      0.50      0.47       984\n",
            "weighted avg       0.76      0.87      0.81       984\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPqQv4JQo4Gs",
        "colab_type": "text"
      },
      "source": [
        "As we can see, transforming the data to its first 250 independent components did not capture all the necessary variability in the data. This has resulted in the degradation of the classification results for this method. We can conclude that ICA is not suitable for this dataset.\n",
        "\n",
        "It was also observed that the time taken to find the best independent features was longer than for PCA. However, it should be noted that different methods vary in results according to the input data. Even though ICA was not suitable for this dataset, it still is a potent method for dimensionality reduction that should be in the repertoire of a data scientist.\n",
        "\n",
        "From this exercise, you may come up with a few questions:\n",
        "\n",
        "How do you think we can improve the classification results using ICA?\n",
        "Increasing the number of components results in a marginal increase in the accuracy metrics.\n",
        "Are there any other side effects because of the strategy adopted to improve the results?\n",
        "Increasing the number of components also results in a longer training time for the logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7twdCaXWpQQX",
        "colab_type": "text"
      },
      "source": [
        "**Factor Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb55B7_tpSFI",
        "colab_type": "text"
      },
      "source": [
        "Factor analysis is a technique that achieves dimensionality reduction by grouping variables that are highly correlated.\n",
        "\n",
        "In our dataset, there could be many features that describe the geometry (the size and shape of an image in the ad) of the images on a web page. These features can be correlated because they refer to specific characteristics of an image.\n",
        "\n",
        "Similarly, there could be many features that describe the anchor text or phrases occurring in a URL, which are highly correlated. Factor analysis looks at correlated groups such as these from the data and then groups them into latent factors. Therefore, if there are 10 raw features describing the geometry of an image, factor analysis will group them into one feature that characterizes the geometry of an image. Each of these groups is called factors. As many correlated features are combined to form a group, the resulting number of features will be much smaller in comparison with the original dimensions of the dataset.\n",
        "\n",
        "Let's now see how factor analysis can be implemented as a technique for dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcUpjx4ipqG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "dcf79783-3ab3-4ada-aaca-7fef32105515"
      },
      "source": [
        "# Creating a high dimension data set\n",
        "X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))\n",
        "\n",
        "print(X_hd.shape)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(3279, 77900)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc0xjXLCpvUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_hd, Y, test_size=0.3, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFtBjae2pwOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the number of factors\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "fa = FactorAnalysis(n_components = 20,random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMccIno-p07f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b82cdcff-94c3-4f1e-83bb-c996c339f768"
      },
      "source": [
        "# Fitting the Factor analysis method and transforming the training set\n",
        "import time\n",
        "t0 = time.time()\n",
        "X_fac=fa.fit_transform(X_train)\n",
        "t1 = time.time()\n",
        "print(\"Factor analysis fitting time:\", round(t1-t0, 3), \"s\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Factor analysis fitting time: 95.624 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O9NBuWfp4WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming the test set\n",
        "X_test_fac=fa.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPqhYzqup7xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7036ecbc-521d-48df-ebc9-a3defece7394"
      },
      "source": [
        "# Printing the shape of train and test sets before and after transformation\n",
        "print(\"original shape of Training set: \", X_train.shape)\n",
        "print(\"original shape of Test set: \", X_test.shape)\n",
        "print(\"Transformed shape of training set:\", X_fac.shape)\n",
        "print(\"Transformed shape of test set:\", X_test_fac.shape)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original shape of Training set:  (2295, 77900)\n",
            "original shape of Test set:  (984, 77900)\n",
            "Transformed shape of training set: (2295, 20)\n",
            "Transformed shape of test set: (984, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDyPnfaZqBXB",
        "colab_type": "text"
      },
      "source": [
        "You can see that both the training and test sets have been reduced to 20 factors each"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHGHf4yiqCBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting a Logistic Regression Model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "facModel = LogisticRegression()\n",
        "t0 = time.time()\n",
        "facModel.fit(X_fac, y_train)\n",
        "t1 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF4ZCUrFqHaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32d1aaa8-098e-467d-a8bc-316196bc0d13"
      },
      "source": [
        "print(\"Total training time:\", round(t1-t0, 3), \"s\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training time: 0.027 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXaNpFhLqKCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73788845-1359-4f1e-e073-1dedac1e4a24"
      },
      "source": [
        "# Predicting with the factor analysis model\n",
        "pred = facModel.predict(X_test_fac)\n",
        "print('Accuracy of Logistic regression model prediction on test set: {:.2f}'.format(facModel.score(X_test_fac, y_test)))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic regression model prediction on test set: 0.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apIwe_FzqNlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7a54ae14-e652-4cdb-9db0-ba359b895ced"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusionMatrix = confusion_matrix(y_test, pred)\n",
        "print(confusionMatrix)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 48  78]\n",
            " [  0 858]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIWi4cOHqPr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fd49ce33-7b43-45b1-ad7e-564fb5e123d8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Getting the Classification_report\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ad.       1.00      0.38      0.55       126\n",
            "      nonad.       0.92      1.00      0.96       858\n",
            "\n",
            "    accuracy                           0.92       984\n",
            "   macro avg       0.96      0.69      0.75       984\n",
            "weighted avg       0.93      0.92      0.90       984\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yle32FhwqP7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZuakDuRqZtv",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the results, by reducing the variables to just 20 factors, we were able to get a decent classification result. Even though there is degradation on the result, we still have a manageable number of features, which will be able to scale well on any algorithm. The balance between the accuracy measures and the ability to manage features needs to be explored through greater experimentation.\n",
        "\n",
        "How do you think we can improve the classification results for factor analysis?\n",
        "\n",
        "Well, increasing the number of components results in an increase in the accuracy metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvo9bL06sDQT",
        "colab_type": "text"
      },
      "source": [
        "we can see that three methods (backward elimination, forward selection, and PCA) have got the same accuracy scores. Therefore, the selection criteria for the best method should be based on the time taken to get the reduced dimension. With these criteria, the forward selection method is the best method, followed by PCA.\n",
        "\n",
        "For the third place, we should strike a balance between accuracy and the time taken for dimensionality reduction. We can see that factor analysis and backward elimination have very close accuracy scores, 96% and 97% respectively. However, the time taken for backward elimination is quite large compared to factor analysis. Therefore, we should weigh our considerations toward factor analysis as the third best, even though the accuracy is marginally lower than backward elimination. The last spot should go to ICA because the accuracy is far lower than all the other methods."
      ]
    }
  ]
}